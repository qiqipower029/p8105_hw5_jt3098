p8105\_hw5\_jt3098
================
Jieqi Tu (jt3098)
11/4/2018

Problem 1
---------

``` r
# Create a data frame that contains all file names
longitudinal_data = 
  tibble(list.files(path = "./data"))

# Rename the column name of the first variable
colnames(longitudinal_data)[1] = 'file'
```

``` r
# Create a function to read in files
read_data = function(file) {
  data = 
    read.csv(str_c("./data/", file)) %>%
    as.data.frame()
  
 data
}

# Load files in using created function above
longitudinal_data =
  longitudinal_data %>%
  mutate(data = map(longitudinal_data$file, read_data)) # All data from one file are nested in one variable called "data"
```

``` r
# Spread out the data set
longitudinal_data = 
  longitudinal_data %>%
  unnest() %>% # unnest the `data` variable to obtain more variables to store the data
  gather(key = week, value = value, week_1:week_8) # make week be a single variable

# Separate the control and experiment groups
longitudinal_data = 
  longitudinal_data %>%
  separate(file, into = c("group_type", "number"), sep = "_") %>%
  separate(number, into = c("ID", "extension"), sep = 2) %>%
  select(-extension)

# Manipulate the "week" variable
longitudinal_data = 
  longitudinal_data %>%
  separate(week, into = c("character", "week_number"), sep = 5) %>%
  select(-character)

# Revise the name of arms
longitudinal_data$group_type = str_replace(longitudinal_data$group_type, "con", "control")
longitudinal_data$group_type = str_replace(longitudinal_data$group_type, "exp", "experimental")

# Change the column type of "week_number"
longitudinal_data$week_number = as.numeric(longitudinal_data$week_number)
```

``` r
# Make a new variable for each subject to have a more clear order
longitudinal_data$subject = str_c(longitudinal_data$group_type, '_', longitudinal_data$ID)

# Make a speghetti plot to show the trend of observation data from each arm
longitudinal_data %>%
  ggplot(aes(x = week_number, y = value, group = subject, color = group_type)) +
  geom_line(alpha = 0.5) + 
  geom_point(alpha = 0.2) +
  labs(
    title = "Observations on Each Subject Over Time",
    x = "Week",
    y = "Observation Data"
  ) +
  theme_bw()
```

![](p8105_hw5_jt3098_files/figure-markdown_github/speghetti%20plot%20for%20problem%201-1.png)

Comments: From this plot, we could know that, in general, experimental groups have higher observational values than control groups. Additionally, the overall trend for experimental groups is increasing over time, while there is relatively no big difference in control groups.

Problem 2
---------

``` r
# Importing data for problem 2
homicide_data = read_csv("./data2/homicide-data.csv")
```

``` r
# Calculate the the number of NAs in each column 
n_NA = 
  sapply(homicide_data, function(x) sum(length(which(is.na(x))))) %>%
  knitr::kable()
n_NA
```

|                |    x|
|----------------|----:|
| uid            |    0|
| reported\_date |    0|
| victim\_last   |    0|
| victim\_first  |    0|
| victim\_race   |    0|
| victim\_age    |    0|
| victim\_sex    |    0|
| city           |    0|
| state          |    0|
| lat            |   60|
| lon            |   60|
| disposition    |    0|

Descriptions about the raw data:

-   There are 52179 rows and 12 columns in this dataset.
-   There are 2999 NA values in "victim\_age" variable, and 60 NAs in "lat" and "lon" respectively.
-   This dataset has 52179 observations.
-   The variables inlude ID, reported date, the last and first name of victims, the sex, race and age of victims, the city and state, longitude and latitude, and the deposition status.

``` r
# Make a new variable to summarize the city and state informtion
homicide_data = 
  homicide_data %>%
  mutate(city_state = str_c(city, ", ", state)) %>%
  select(-city, -state)
```

``` r
# Calculate the total number of cases of homicide
homicide_data_summary = 
  homicide_data %>%
  group_by(city_state) %>%
  mutate(n_homicide = n())

# Calculate the total number of unsolved cases
homicide_data_summary = 
  homicide_data_summary %>%
  filter(disposition %in% c("Open/No arrest", "Closed without arrest")) %>%
  group_by(city_state) %>%
  mutate(n_unsolved = n())

# Summarize the total number of cases and unsolved cases by city and state
homicide_data_summary = 
  homicide_data_summary %>%
  distinct(city_state, n_homicide, n_unsolved) %>%
  as.data.frame()

# Check the number of cases and unsolved
homicide_data_summary
```

    ##            city_state n_homicide n_unsolved
    ## 1     Albuquerque, NM        378        146
    ## 2         Atlanta, GA        973        373
    ## 3       Baltimore, MD       2827       1825
    ## 4     Baton Rouge, LA        424        196
    ## 5      Birmingham, AL        800        347
    ## 6          Boston, MA        614        310
    ## 7         Buffalo, NY        521        319
    ## 8       Charlotte, NC        687        206
    ## 9         Chicago, IL       5535       4073
    ## 10     Cincinnati, OH        694        309
    ## 11       Columbus, OH       1084        575
    ## 12         Dallas, TX       1567        754
    ## 13         Denver, CO        312        169
    ## 14        Detroit, MI       2519       1482
    ## 15         Durham, NC        276        101
    ## 16     Fort Worth, TX        549        255
    ## 17         Fresno, CA        487        169
    ## 18        Houston, TX       2942       1493
    ## 19   Indianapolis, IN       1322        594
    ## 20   Jacksonville, FL       1168        597
    ## 21    Kansas City, MO       1190        486
    ## 22      Las Vegas, NV       1381        572
    ## 23     Long Beach, CA        378        156
    ## 24    Los Angeles, CA       2257       1106
    ## 25     Louisville, KY        576        261
    ## 26        Memphis, TN       1514        483
    ## 27          Miami, FL        744        450
    ## 28      Milwaukee, wI       1115        403
    ## 29    Minneapolis, MN        366        187
    ## 30      Nashville, TN        767        278
    ## 31    New Orleans, LA       1434        930
    ## 32       New York, NY        627        243
    ## 33        Oakland, CA        947        508
    ## 34  Oklahoma City, OK        672        326
    ## 35          Omaha, NE        409        169
    ## 36   Philadelphia, PA       3037       1360
    ## 37        Phoenix, AZ        914        504
    ## 38     Pittsburgh, PA        631        337
    ## 39       Richmond, VA        429        113
    ## 40    San Antonio, TX        833        357
    ## 41     Sacramento, CA        376        139
    ## 42       Savannah, GA        246        115
    ## 43 San Bernardino, CA        275        170
    ## 44      San Diego, CA        461        175
    ## 45  San Francisco, CA        663        336
    ## 46      St. Louis, MO       1677        905
    ## 47       Stockton, CA        444        266
    ## 48          Tampa, FL        208         95
    ## 49          Tulsa, OK        583        193
    ## 50     Washington, DC       1345        589

``` r
# Make a proportion test for unsolved cases in the total cases
prop.test(x = homicide_data_summary$n_unsolved[homicide_data_summary$city_state == "Baltimore, MD"],
          n = homicide_data_summary$n_homicide[homicide_data_summary$city_state == "Baltimore, MD"],
          alternative = "two.sided")
```

    ## 
    ##  1-sample proportions test with continuity correction
    ## 
    ## data:  homicide_data_summary$n_unsolved[homicide_data_summary$city_state ==  out of homicide_data_summary$n_homicide[homicide_data_summary$city_state == , null probability 0.5    "Baltimore, MD"] out of     "Baltimore, MD"], null probability 0.5
    ## X-squared = 239.01, df = 1, p-value < 2.2e-16
    ## alternative hypothesis: true p is not equal to 0.5
    ## 95 percent confidence interval:
    ##  0.6275625 0.6631599
    ## sample estimates:
    ##         p 
    ## 0.6455607
